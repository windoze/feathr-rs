/*
 * Jobs API 2.1
 *
 * The Jobs API allows you to create, edit, and delete jobs.
 *
 * The version of the OpenAPI document: 2.1
 * 
 * Generated by: https://openapi-generator.tech
 */




#[derive(Clone, Debug, PartialEq, Default, Serialize, Deserialize)]
pub struct SparkJarTask {
    /// The full name of the class containing the main method to be executed. This class must be contained in a JAR provided as a library.  The code must use `SparkContext.getOrCreate` to obtain a Spark context; otherwise, runs of the job fail.
    #[serde(rename = "main_class_name", skip_serializing_if = "Option::is_none")]
    pub main_class_name: Option<String>,
    /// Parameters passed to the main method.  Use [Task parameter variables](https://docs.microsoft.com/azure/databricks/jobs#parameter-variables) to set parameters containing information about job runs.
    #[serde(rename = "parameters", skip_serializing_if = "Option::is_none")]
    pub parameters: Option<Vec<String>>,
    /// Deprecated since 04/2016\\. Provide a `jar` through the `libraries` field instead. For an example, see [Create](https://docs.microsoft.com/azure/databricks/dev-tools/api/latest/jobs#operation/JobsCreate).
    #[serde(rename = "jar_uri", skip_serializing_if = "Option::is_none")]
    pub jar_uri: Option<String>,
}

impl SparkJarTask {
    pub fn new() -> SparkJarTask {
        SparkJarTask {
            main_class_name: None,
            parameters: None,
            jar_uri: None,
        }
    }
}


