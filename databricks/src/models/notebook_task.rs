/*
 * Jobs API 2.1
 *
 * The Jobs API allows you to create, edit, and delete jobs.
 *
 * The version of the OpenAPI document: 2.1
 * 
 * Generated by: https://openapi-generator.tech
 */




#[derive(Clone, Debug, PartialEq, Default, Serialize, Deserialize)]
pub struct NotebookTask {
    /// The absolute path of the notebook to be run in the Azure Databricks workspace. This path must begin with a slash. This field is required.
    #[serde(rename = "notebook_path")]
    pub notebook_path: String,
    /// Base parameters to be used for each run of this job. If the run is initiated by a call to [`run-now`](https://docs.microsoft.com/azure/databricks/dev-tools/api/latest/jobs#operation/JobsRunNow) with parameters specified, the two parameters maps are merged. If the same key is specified in `base_parameters` and in `run-now`, the value from `run-now` is used.  Use [Task parameter variables](https://docs.microsoft.com/azure/databricks/jobs#parameter-variables) to set parameters containing information about job runs.  If the notebook takes a parameter that is not specified in the jobâ€™s `base_parameters` or the `run-now` override parameters, the default value from the notebook is used.  Retrieve these parameters in a notebook using [dbutils.widgets.get](https://docs.microsoft.com/azure/databricks/dev-tools/databricks-utils#dbutils-widgets).
    #[serde(rename = "base_parameters", skip_serializing_if = "Option::is_none")]
    pub base_parameters: Option<::std::collections::HashMap<String, serde_json::Value>>,
}

impl NotebookTask {
    pub fn new(notebook_path: String) -> NotebookTask {
        NotebookTask {
            notebook_path,
            base_parameters: None,
        }
    }
}


